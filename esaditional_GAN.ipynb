{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_apples = []\n",
    "green_apples = []\n",
    "\n",
    "for directory in os.listdir(r\"C:\\Python_Projects\\esaditional_GAN\\conditional-GAN\\datasett\"):\n",
    "    path = os.path.join(r\"C:\\Python_Projects\\esaditional_GAN\\conditional-GAN\\datasett\", directory)\n",
    "    for image_name in os.listdir(path):\n",
    "        image_path = os.path.join(path, image_name)\n",
    "        image = Image.open(image_path)\n",
    "        image = image.convert(\"RGB\")\n",
    "        if (directory == \"red_apples\"):\n",
    "            img = np.asarray(image)\n",
    "            red_apples.append(img)\n",
    "        elif (directory == \"green_apples\"):\n",
    "            img = np.asarray(image)\n",
    "            green_apples.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = red_apples[1]\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator and Discriminator blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, layer_sizes: list, noise_size: int, sat_im_size: list, ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.im_size = sat_im_size[0]*sat_im_size[1]\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(),\n",
    "            nn.Conv2d(),\n",
    "            nn.Dropout(),\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, noise, lable_image):\n",
    "        noise = noise.view()\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
